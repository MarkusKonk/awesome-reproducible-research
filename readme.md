# Awesome Reproducible Research [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3564746.svg)](https://doi.org/10.5281/zenodo.3564746)


<meta property="og:image" content="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2.png?raw=true"/>
<img src="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2.png?raw=true" align="right" width="200">

> A curated list of reproducible research case studies, projects, tutorials, and media


## Contents

- [Case studies](#case-studies)
- [Ad-hoc reproductions](#ad-hoc-reproductions)
- [Theory papers](#theory-papers)
- [Courses](#courses)
- [Development Resources](#development-resources)
- [User tools](#user-tools)
- [Books](#books)
- [Data Repositories](#data-repositories)
- [Examples and exemplars](#examples-and-exemplars)
- [Journals](#journals)
- [Ontologies](#ontologies)
- [Organizations](#organizations)
- [Awesome Lists](#awesome-lists)

## Case studies
The term "case studies" is used here in a general sense to describe any study of reproducibility. A _reproduction_ is an attempt to arrive at comparable results with identical data using computational methods described in a paper. A _refactor_ involves refactoring existing code into frameworks and other reproducibility best practices while preserving the original data. A _replication_ involves generating new data and applying existing methods to achieve comparable results. A _robustness test_ applies various protocols, workflows, statistical models or parameters to a given data set to study their effect on results, either as a follow-up to an existing study or as a "bake-off". A _census_ is a high-level tabulation conducted by a third party. A _survey_ is a questionnaire sent to practitioners. A _case narrative_ is an in-depth first-person account. An _independent discussion_ utilizes a secondary independent author to interpret the results of a study as a means to improve inferential reproducibility.
<table id="case_studies">
			<tbody>
				<tr>
					<td>
						<p>
							Study
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Approach
						</p>
					</td>
					<td>
						<p>
							Size
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1136/bmj.39590.732037.47"><span title="What is missing from descriptions of treatment in trials and reviews?">Glasziou et al <meta property="datePublished" content="2008-06-26">2008</span></a>
						</p>
					</td>
					<td>
						<p>
							Medicine
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							80 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1214/09-AOAS291"><span title="DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY">Baggerly &amp; Coombes <meta property="datePublished" content="2009-09-01">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							8 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.200900154"><span title="Biometrical Journal and Reproducible Research">Hothorn et al. <meta property="datePublished" content="2009-08-17">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Biostatistics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							56 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/ng.295"><span title="Repeatability of published microarray gene expression analyses">Ioannidis et al <meta property="datePublished" content="2009-01-28">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Genetics
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.uio.no/studier/emner/matnat/ifi/INF5700/h11/undervisningsmateriale/Anda.Sj%C3%B8berg.Mockus.TSE.May.2009.pdf">Anda et al <meta property="datePublished" content="2009-07-30">2009</a>
						</p>
					</td>
					<td>
						<p>
							Software engineering
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							4 companies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://infoscience.epfl.ch/record/136640/files/VandewalleKV09.pdf">Vandewalle et al <meta property="datePublished" content="2009-04-22">2009</a>
						</p>
					</td>
					<td>
						<p>
							Signal processing
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							134 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/nrd3439-c1">Prinz <meta property="datePublished" content="2011-08-31">2011</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							23 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://academic.oup.com/bib/article/12/3/288/258098/Case-studies-in-reproducibility">Horthorn &amp; Leisch <meta property="datePublished" content="2011-01-28">2011</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/nature/journal/v483/n7391/full/483531a.html">Begley &amp; Ellis <meta property="datePublished" content="2012-03-29">2012</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							53 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://reproducibility.cs.arizona.edu/tr.pdf">Collberg et al <meta property="datePublished" content="2014-03-21">2014</a><br/><a href="https://sci-hub.tw/10.1145/2812803">Collberg &amp; Proebsting 2016</a>
						</p>
					</td>
					<td>
						<p>
							Computer science
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							613 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/349/6251/aac4716">OSC <meta property="datePublished" content="2015-08-28">2015</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://f1000research.com/articles/4-134/v2">Bandrowski et al <meta property="datePublished" content="2015-05-29">2015</a>
					</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 papers
						</p>
					</td>
				</tr>
								<tr>
					<td>
					<p>
					<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4555355/">Patel et al <meta property="datePublished" content="2015-06-06">2015</a>
					</p>
					</td>
					<td>
						<p>
							Epidemiology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							417 variables
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://doi.org/10.1038/533452a"><span title="1,500 scientists lift the lid on reproducibility">Baker <meta property="datePublished" content="2016-05-26">2016</span></a>
					</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							<span title="More than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research.">Survey</span>
						</p>
					</td>
					<td>
						<p>
							1,576 researchers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://pdfs.semanticscholar.org/edd7/e68711955cbbdb6dd6866db2ec8a6ff18278.pdf">Névéol et al <meta property="datePublished" content="2016-11-05">2016</a>
						</p>
					</td>
					<td>
						<p>
							NLP
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							3 studies
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://elifesciences.org/articles/23383#abstract">Reproducibility Project <meta property="datePublished" content="2017-01-19">2017</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							9 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="https://peerj.com/articles/3208/">Vasilevsky et al <meta property="datePublished" content="2017-04-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							318 journals
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="https://www.practicereproducibleresearch.org/">Kitzes et al <meta property="datePublished" content="2017-10-17">2017</a>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							31 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005755">Barone et al <meta property="datePublished" content="2017-10-19">2017</a>
						</p>
					</td>
					<td>
						<p>
						        Biological sciences
						<p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							704 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/biorxiv/early/2017/10/31/143503.full.pdf?%3Fcollection=">Kim &amp; Dumas <meta property="datePublished" content="2017-10-31">2017</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/351/6280/1433">Camerer <meta property="datePublished" content="2016-03-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://openreview.net/pdf?id=By4l2PbQ-">Olorisade <meta property="datePublished" content="2017-08-06">2017</a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1515/opar-2017-0019">Strupler &amp; Wilkinson <meta property="datePublished" content="2017-11-14">2017</a>
						</p>
					</td>
					<td>
						<p>
							Archaeology
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 survey
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/1801.05042">Danchev et al <meta property="datePublished" content="2018-01-15">2017</a>
						</p>
					</td>
					<td>
						<p>
							Comparative toxicogenomics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							51,292 claims in 3,363 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248">Kjensmo &amp; Gundersen <meta property="datePublished" content="2018-04-25">2018</a>
						</p>
					</td>
					<td>
						<p>
							Artificial intelligence
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							400 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/articles/d41586-018-02108-9">Gertler et al <meta property="datePublished" content="2018-02-21">2018</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							203 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.pnas.org/content/115/11/2584">Stodden et al <meta property="datePublished" content="2018-03-13">2018</a>
						</p>
					</td>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							204 articles, 180 authors
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41562-018-0399-z">Camerer et al <meta property="datePublished" content="2018-08-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							21 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1177/2515245917747646">Silberzahn et al <meta property="datePublished" content="2018-08-23">2018</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							One data set, 29 analyst teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.201700243">Boulesteix et al <meta property="datePublished" content="2018-08-01">2018</a>
						</p>
					</td>
					<td>
						<p>
							Medicine and health sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7554/eLife.34364">Eaton et al <meta property="datePublished" content="2018-10-03">2018</a>
						</p>
					</td>
					<td>
						<p>
							Microbiome immuno oncology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/early/2018/11/08/463927">Vaquero-Garcia et al <meta property="datePublished" content="2018-11-08">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor and test of robustness
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.2006930">Wallach et al <meta property="datePublished" content="2018-11-20">2018</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical Sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							149 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://broad.io/ASHG2018">Miller et al <meta property="datePublished" content="2018-10-18">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Synthetic replication & refactor
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/13658816.2018.1508687">Konkol et al <meta property="datePublished" content="2018-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey, Reproduction
						</p>
					</td>
					<td>
						<p>
							146 scientists, 41 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://amid.fish/reproducing-deep-rl">Rahtz <meta property="datePublished" content="2018-04-06">2018<a>
								</p>
								</td>
							<td>
								<p>
									Reinforcement Learning
								</p>
							</td>
							<td>
								<p>
									Reproduction, case narrative
								</p>
							<td>
								<p>
									1 paper
								</p>
							</td>
						</p>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://peerj.com/articles/cs-163/">AlNoamany & Borghi <meta property="datePublished" content="2018-09-17">2018</a>
						</p>
					</td>
					<td>
						<p>
							Science & Engineering
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							215 participants
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1155/2018/4789035
">Li et al <meta property="datePublished" content="2018-09-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Nephrology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://dash.harvard.edu/bitstream/handle/1/38811561/CHEN-SENIORTHESIS-2018.pdf?sequence=3
">Chen <meta property="datePublished" content="2018-06-29">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences & other
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							810 Dataverse studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/sdata.2019.30">Stagge et al <meta property="datePublished" content="2019-02-26">2019</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							360 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1006269">Bizzego et al <meta property="datePublished" content="2019-03-27">2019</a>
						</p>
					</td>
					<td>
						<p>
							Deep learning
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-11">2019</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<span title="Creating reproducible pharmacogenomic analysis pipelines"><a href="10.1038/s41597-019-0174-7">Mammoliti
								et al <meta property="datePublished" content="2019-09-03">2019</a></span>
						</p>
					</td>
					<td>
						<p>
							Pharmacogenomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							2 analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1371/journal.pbio.3000246"><span title="Open science challenges, benefits and tips in early career and beyond">Allen & Mehler <meta property="datePublished" content="2019-05-01">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences and Psychology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							127 registered reports 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="http://www.ic.uff.br/~leomurta/papers/pimentel2019a.pdf"><span title="A Large-scale Study about Quality and
Reproducibility of Jupyter Notebooks">Pimentel et al <meta property="datePublished" content="2019-05-07">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							All
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 1,159,166 Jupyter notebooks 
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.omto.2019.05.004"><span title="Assessing the Completeness of Reporting in Preclinical Oncolytic Virus Therapy Studies">Fergusson et al <meta property="datePublished" content="2019-05-20">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Virology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 236 papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.bja.2019.01.012
"><span title="Hypnotic depth and postoperative death: a Bayesian perspective and an Independent Discussion of a clinical trial">Vlisides et al <meta property="datePublished" content="2019-01-22">2019</span></a><br/><a href="https://doi.org/10.1016/j.bja.2018.12.021"><span title="Depth of sedation as an interventional target to reduce postoperative delirium: mortality and functional outcomes of the Strategy to Reduce the Incidence of Postoperative Delirium in Elderly Patients randomised clinical trial">Sieber et al <meta property="datePublished" content="2019-01-22">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Anaesthesia
						</p>
					</td>
					<td>
						<p>
							Indepedent discussion
						</p>
					</td>
					<td>
						<p>
							 1 study 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://osf.io/7yt8u/
"><span title="Replication Oxley et al. (2008, Science)">Bakker et al <meta property="datePublished" content="2019-06-19">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							 1 paper 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.cels.2019.06.005
"><span title="A Multi-center Study on the Reproducibility of Drug-Response Assays in Mammalian Cell Lines">Niepel et al <meta property="datePublished" content="2019-07-10">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Cell pharmacology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							 5 labs 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://arxiv.org/abs/1907.06902v1"><span title="
Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches">Dacrema et al <meta property="datePublished" content="2019-16-07">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 conference papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.jasrep.2019.102002"><span title="Experimental replication shows knives manufactured from frozen human feces do not work">Eran et al <meta property="datePublished" content="2019-10-09">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Experimental archaeology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 theory
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/763730"><span title="Reproducible and Transparent Research Practices in Published Neurology Research">Rauh et al <meta property="datePublished" content="2019-16-09">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							202 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/dkg53"><span title="Failed pre-registered replication of mortality salience effects in traditional and novel measures">Sætrevik & Sjåstad <meta property="datePublished" content="2019-20-09">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							2 experiments
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41559-019-0972-5"><span title="A checklist for maximizing reproducibility of ecological niche models">Feng et al. <meta property="datePublished" content="2019-09-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Ecology and Evolution
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							163 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/843193"><span title="Variability in the analysis of a single neuroimaging dataset by many teams">Botvinik-Nezer et al. <meta property="datePublished" content="2019-15-11">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neuroimaging
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 data set, 70 teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/vef2c"><span title="Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement">Klein et al. <meta property="datePublished" content="2019-12-11">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Many Labs 4: Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 21 Labs and N = 2,220 participants) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not yet understood or no longer exist.">Replication</span>
						</p>
					</td>
					<td>
						<p>
							1 experiment, 21 labs, 2,220 participants
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/fk8vh"><span title="Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology">Obels et al. <meta property="datePublished" content="2019-05-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. However, these benefits will only emerge if researchers can reproduce the analysis reported in published articles and if data is annotated well enough so that it is clear what all variables mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature between 2014 and 2018, and attempted to independently computationally reproduce the main results in each article. Of the main results from 62 articles that met our inclusion criteria, data were available for 41 articles, and analysis scripts for 37 articles. For the main results in 36 articles that shared both data and code we could run the scripts for 31 analyses, and reproduce the main results for 21 articles. Although the articles that shared both data and code (36 out of 62, or 58%) and articles for which main results could be computationally reproduced (21 out of 36, or 58%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations and link to examples of good research practices in the papers we reproduced.">Census</span>
						</p>
					</td>
					<td>
						<p>
							62 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/2020.01.30.924092"><span title="Factorial study of the RNA-seq computational workflow identifies biases as technical gene signatures">Simoneau et al. <meta property="datePublished" content="2020-01-30">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							<span title="RNA-seq is a modular experimental and computational approach that aims in identifying and quantifying RNA molecules. The modularity of the RNA-seq technology enables adaptation of the protocol to develop new ways to explore RNA biology, but this modularity also brings forth the importance of methodological thoroughness. Liberty of approach comes with the responsibility of choices, and such choices must be informed. Here, we present an approach that identifies gene group specific quantification biases in currently used RNA-seq software and references by processing sequenced datasets using a wide variety of RNA-seq computational pipelined, and by decomposing these expression datasets using an independent component analysis matrix factorisation method. By exploring the RNA-seq pipeline using a systemic approach, we highlight the yet inadequately characterized central importance of genome annotations in quantification results. We also show that the different choices in RNA-seq methodology are not independent, through interactions between genome annotations and quantification software. Genes were mainly found to be affected by differences in their sequence, by overlapping genes and genes with similar sequence. Our approach offers an explanation for the observed biases by identifying the common features used differently by the software and references, therefore providing leads for the betterment of RNA-seq methodology.">Robustness test</span>
						</p>
					</td>
					<td>
						<p>
							1 data set
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/2020.01.30.924092"><span title="No raw data, no science: another possible source of the reproducibility crisis">Miyakawa <meta property="datePublished" content="2020-02-20">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurobiology
						</p>
					</td>
					<td>
						<p>
							<span title="A reproducibility crisis is a situation where many scientific studies cannot be reproduced. Inappropriate practices of science, such as HARKing, p-hacking, and selective reporting of positive results, have been suggested as causes of irreproducibility. In this editorial, I propose that a lack of raw data or data fabrication is another possible cause of irreproducibility. As an Editor-in-Chief of Molecular Brain, I have handled 180 manuscripts since early 2017 and have made 41 editorial decisions categorized as Revise before review, requesting that the authors provide raw data. Surprisingly, among those 41 manuscripts, 21 were withdrawn without providing raw data, indicating that requiring raw data drove away more than half of the manuscripts. I rejected 19 out of the remaining 20 manuscripts because of insufficient raw data. Thus, more than 97% of the 41 manuscripts did not present the raw data supporting their results when requested by an editor, suggesting a possibility that the raw data did not exist from the beginning, at least in some portions of these cases. Considering that any scientific study should be based on raw data, and that data storage space should no longer be a challenge, journals, in principle, should try to have their authors publicize raw data in a public database or journal site upon the publication of the paper to increase reproducibility of the published results and to increase public trust in science.">Census</span>
						</p>
					</td>
					<td>
						<p>
							41 papers
						</p>
					</td>
				</tr>
			</tbody>
		</table>

## Ad-hoc reproductions

These are one-off unpublished attempts to reproduce individual studies

<table id="ad_hoc">
			<tbody>
				<tr>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							Original study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://rdoodles.rbind.io/2019/06/reanalyzing-data-from-human-gut-microbiota-from-autism-spectrum-disorder-promote-behavioral-symptoms-in-mice/
							and
							https://notstatschat.rbind.io/2019/06/16/analysing-the-mouse-autism-data/
						</p>
					</td>
					<td>
						<p>
							Sharon, G. et al. Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice. Cell 2019, 177 (6), 1600–1618.e17.
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://github.com/sean-harrison-bristol/CCR5_replication
						</p>
					</td>
					<td>
						<p>
							Wei, X.; Nielsen, R. CCR5-∆32 Is Deleterious in the Homozygous State in Humans. Nat. Med. 2019 DOI: 10.1038/s41591-019-0459-6. (retracted)
						</p>
					</td>
				</tr>
				</tbody>
		</table>
			


## Theory papers
<table>
	<tbody>
				<tr>
					<td>
						<p>
							Authors/Date
						</p>
					</td>
					<td>
						<p>
							Title
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Type
						</p>
					</td>
				</tr>
		<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/09332480.2019.1579573">Ioannidis <meta property="datePublished" content="2005-08-30">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.">Why most published research findings are false</span>
						</p>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
			</td>	
			</td>
				</tr>
						<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Noble <meta property="datePublished" content="2009-07-31">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Most bioinformatics coursework focuses on algorithms, with perhaps some components devoted to learning programming skills and learning how to use existing bioinformatics software. Unfortunately, for students who are preparing for a research career, this type of curriculum fails to address many of the day-to-day organizational challenges associated with performing computational experiments. In practice, the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections as well as by chance interactions with collaborators or colleagues. The purpose of this article is to describe one good strategy for carrying out computational experiments. I will not describe profound issues such as how to formulate hypotheses, design experiments, or draw conclusions. Rather, I will focus on relatively mundane issues such as organizing files and directories and documenting progress. These issues are important because poor organizational choices can lead to significantly slower research progress. I do not claim that the strategies I outline here are optimal. These are simply the principles and practices that I have developed over 12 years of bioinformatics research, augmented with various suggestions from other researchers with whom I have discussed these issues.">A Quick Guide to Organizing Computational Biology Projects</span>
						</p>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
										<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Sandve et al <meta property="datePublished" content="2013-10-24">2013</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4].">Ten Simple Rules for Reproducible Computational Research</span>
						</p>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31234/osf.io/jqw35">Yarkoni <meta property="datePublished" content="2019-11-21">2019</a>
						</p>
					</td>
			<td>
						<p>
							<span title="Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned—that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that most inferential statistical tests in psychology fail to meet this basic condition. I demonstrate how foundational assumptions of the "random effects" model used pervasively in psychology impose far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints dramatically inflates false positive rates and routinely leads researchers to draw sweeping verbal generalizations that lack any meaningful connection to the statistical quantities they are putatively based on. I argue that the routine failure to consider the generalizability of one's conclusions from a statistical perspective lies at the root of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.">The Generalizability Crisis</span>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
						<td>
						<p>
							Statistical reproducibility
						</p>
			</td>
				</tr>
				<tr>
		<td>
			<p>
				<a href="http://proceedings.mlr.press/v97/bouthillier19a.html">Bouthillier et al <meta property="datePublished" content="2019-06-09">2019</a>
</p>
					</td>
			<td>
						<p>
<span title="The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.">Unreproducible Research is Reproducible</span>
</p>
					</td>
					<td>
						<p>
							Machine Learning
						</p>
					</td>
						<td>
						<p>
							Methodology
						</p>
			</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41567-019-0780-5">Milton & Possolo <meta property="datePublished" content="2019-06-09">2019</a>
	</td><td>
						<p><span title="Lack of reproducibility is not necessarily bad news; it may herald new discoveries and signal scientifc progress">Trustworthy data underpin reproducible research</span></p>
	</td>
	<td>
						<p>
							Physics
						</p>
					</td>
						<td>
						<p>
							Scientific philosophy
						</p>
			</td>
				</tr>					
	</tbody>
</table>

## Courses
- MOOCs
    - [Coursera Reproducible Research](https://www.coursera.org/learn/reproducible-research) - Roger Peng et al JHU. Very popular course.
    - [edX Principles, Statistical and Computational Tools for Reproducible Science](https://www.edx.org/course/principles-statistical-computational-harvardx-ph527x) - John Quackenbush et al Harvard
- Online course content
    - [Tools for Reproducible Research](http://kbroman.org/Tools4RR/) - Karl Broman UW, includes resources page
    - [R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/) - Software Carpentry workshop primer using Gapminder data
    - [R-DAVIS](https://gge-ucd.github.io/R-DAVIS/syllabus.html) - Student-developed computer literacy and data course in R
    - [AMIA2019](https://github.com/StatTag/amia-2019-spring-rr/) - Pragmatic RR for Analysis, Dissemination and Publication

## Development Resources
- R
    - [CRAN Task View - Reproducible Research](https://cran.r-project.org/web/views/ReproducibleResearch.html) - packages relevant to RCR in R
    - [liftr](https://liftr.me/) - persistent reproducible reporting through containerized R Markdown documents
    - [repo](https://github.com/franapoli/repo) - provenance framework package

## User tools
- Open With Binder for [Chrome](https://matthiasbussonnier.com/posts/32-open-with-binder-chrome.html) or [Firefox](https://addons.mozilla.org/en-US/firefox/addon/open-with-binder/) - open the GitHub repository you are visiting using MyBinder.org
- [DVC](https://dvc.org/) - DVC tracks machine learning models and data sets

## Books
- [Reproducible Research with R and R Studio 2013](https://g.co/kgs/RxcFNm)
- [Implementing Reproducible Research 2014](https://osf.io/s9tya/) - Describes projects: Sumatra, Vistrails, CDE, SOLE, JUMBO, CML, knitr. Content available on OSF.
- [The Practice of Reproducible Research 2017](https://g.co/kgs/jZiMR7) - 31 first person case narratives and intro chapters
- [Dynamic Documents with R and knitr 2015](https://g.co/kgs/dpzkF4)
- [The Turing Way: A Handbook for Reproducible Data Science 2020](https://the-turing-way.netlify.com/introduction/introduction)

## Data Repositories
All these repositories assign Digital Object Identifiers (DOIs) to data
- [DataCite](https://datacite.org) - 12M+ DOIs registered for 46 allocators. Offers APIs and a metadata schema.
- [Data Dryad](https://datadryad.org) - curated, metadata-centric, focused on articles associated with published artices, $120 submission fee (various waivers available)
- [Figshare](https://figshare.com) - 20 GB of free private space, unlimited public space, >2M articles, >5k projects
- [OSF](https://osf.io) - Project-oriented system with access control and integration with popular tools. Unlimited storage for projects, but individual files are limited to 5 gigabytes (GB) each.
- [Zenodo](https://zenodo.org/) - Allows embargoed, restricted access, metadata support. 50GB limit.

## Examples and Exemplars
- [Jupyter Gallery](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks) - Gallery of interesting Jupyter notebooks
- [Papers With Code](https://paperswithcode.com/) - ML papers with code
- [NARPS](https://github.com/poldrack/narps) - Code related to Neuroimaging Analysis Replication and Prediction Study

### Haibe-Kains lab reproducible papers
<table>
    <tr>
        <th>
            Publication
        </th>
        <th>
            CodeOcean link
        </th>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/471227">Mer AS et al. Integrative Pharmacogenomics Analysis of Patient Derived Xenografts</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0566399">codeocean.com/capsule/056639</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/052910">Gendoo, Zon et al. MetaGxData: Clinically Annotated Breast, Ovarian and Pancreatic Cancer Datasets and their Use in Generating a Multi-Cancer Gene Signature</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6438633">codeocean.com/capsule/643863</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1093/jamia/ocx062">Yao et al. Tissue specificity of in vitro drug sensitivity</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/5502756">codeocean.com/capsule/550275</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1038/s41467-017-02136-5">Safikhani Z et al. Gene isoforms as expression-based biomarkers predictive of drug response in vitro</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0002901">codeocean.com/capsule/000290</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1158/0008-5472.CAN-17-0096">El-Hachem et al. Integrative cancer pharmacogenomics to infer large-scale drug taxonomy</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/4252248">codeocean.com/capsule/425224</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.12688/f1000research.9611.3">Safikhani Z et al. Revisiting inconsistency in large pharmacogenomic studies</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6276064">codeocean.com/capsule/627606</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/355602">Sandhu V et al. Meta-analysis of 1,200 transcriptomic profiles identifies a prognostic model for pancreatic ductal adenocarcinoma</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/2693620">codeocean.com/capsule/269362</a>
        </td>
    </tr>
</table>


## Journals
- [ReScience](http://rescience.github.io/) - Journal dedicated to insilico reproductions and tests of robustness, lives on Github.
- [ReplicationWiki](http://replication.uni-goettingen.de/wiki/index.php) - Replication in the social sciences, particularly economics

## Ontologies
- [FAIRsharing](https://fairsharing.org) - standards, databases, and policies
- [BioPortal](https://bioportal.bioontology.org/) - 660 biomedical ontologies 

## Organizations
- [ResearchObject.org](http://www.researchobject.org/) - RO specifications and publications
- [BioCompute](https://osf.io/zm97b/) - BCO specs
- [rOpenSci](https://ropensci.org) - Tools, conferences, and education
- [Open Science Framework](https://osf.io) - Open source project management
- [pyOpenSci](https://www.pyopensci.org/) - Promotes open and reproducible research through peer-review of scientific Python packages
- [Replication Network](https://replicationnetwork.com/) - Furthering the practice of replication in economics. Econ replication database.

## Awesome Lists
- [Awesome Pipeline](https://github.com/pditommaso/awesome-pipeline) - So many pipelines frameworks
- [Awesome Docker](https://github.com/veggiemonk/awesome-docker) - Everything related to the Docker containerization system
- [Awesome R](https://github.com/qinwf/awesome-R#reproducible-research) - Section on RR tools
- [Awesome Jupyter](https://github.com/adebar/awesome-jupyter) - Jupyter projects, libraries and resources
- [Awesome Bioinformatics Benchmarks](https://github.com/j-andrews7/Awesome-Bioinformatics-Benchmarks) - Benchmarks are a related aspect of robustness testing
- [Awesome Open Science](https://github.com/ZoranPandovski/awesome-open-science) - Resources, data, tools, and scholarship
- [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets) - A topic-centric list of HQ open datasets

## Contribute

Contributions welcome! Read the [contribution guidelines](contributing.md) first.


## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](http://creativecommons.org/publicdomain/zero/1.0)

To the extent possible under law, Jeremy Leipzig has waived all copyright and
related or neighboring rights to this work.
